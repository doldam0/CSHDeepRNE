{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "#   잔반량 예측 알고리즘   #\n",
    "############################\n",
    "\n",
    "# Import package\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from matplotlib import pyplot as plt\n",
    "from csv_to_list import get_data\n",
    "\n",
    "\n",
    "# Data\n",
    "x_train, y_train, x_test, y_test = get_data('data_classify.csv', 0.8)\n",
    "\n",
    "# Design\n",
    "model = Sequential()\n",
    "num_of_hidden = 3\n",
    "hidden_layer = 8\n",
    "model.add(Dense(units=hidden_layer, input_dim=11, activation='relu'))\n",
    "for i in range(num_of_hidden):\n",
    "    model.add(Dense(units=hidden_layer, activation='relu'))\n",
    "model.add(Dense(units=5, activation='softmax'))\n",
    "\n",
    "# Compile\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fitting\n",
    "this_epochs = 500\n",
    "this_batch_size = 25\n",
    "hist = model.fit(x_train, y_train, epochs=this_epochs, batch_size=this_batch_size)\n",
    "\n",
    "# Process\n",
    "print(' ## GOD PARK GEUP SICK ## ')\n",
    "print(hist.history['loss'])\n",
    "print(hist.history['acc'])\n",
    "\n",
    "# Evaluate\n",
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=this_batch_size)\n",
    "print(' ## GOD PARK GEUP SICK IS BEING EVALUATED ## ')\n",
    "print('loss and metrics')\n",
    "print(loss_and_metrics)\n",
    "\n",
    "# Save\n",
    "print(' ## Now Saving... ## ')\n",
    "json_string = model.to_json()\n",
    "model.save('park_sick.h5')\n",
    "print(' ## ...Success! ## ')\n",
    "\n",
    "# Graph\n",
    "plt.title('Loss')\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.show()\n",
    "\n",
    "plt.title('Accuracy')\n",
    "plt.plot(hist.history['acc'], c='y')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "#   PCA 알고리즘   #\n",
    "####################\n",
    "\n",
    "# Import Package\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from csv_to_list import get_data\n",
    "\n",
    "\n",
    "X, y, t1, t2 = get_data('data_classify.csv', 1)\n",
    "\n",
    "standardizedData = StandardScaler().fit_transform(X, y)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "principalComponents = pca.fit_transform(X=standardizedData)\n",
    "\n",
    "# to get how much variance was retained\n",
    "print(pca.explained_variance_ratio_.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "#   Boosting 알고리즘   #\n",
    "#########################\n",
    "\n",
    "# Import Package\n",
    "from xgboost import XGBClassifier, plot_importance\n",
    "from matplotlib import pyplot\n",
    "from csv_to_list import get_data\n",
    "\n",
    "\n",
    "X, y, t1, t2 = get_data('data_classify.csv', 1)\n",
    "\n",
    "model = XGBClassifier()\n",
    "model.fit(X, y)\n",
    "plot_importance(model)\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "#   식단 제작 알고리즘   #\n",
    "##########################\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
